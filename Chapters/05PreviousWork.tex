
Data extraction and indexing from various blockchains is an essential topic for making Web3 and dApps working, there are different projects that have tried to address this problem. A lot of work has been done by companies, whose source code and methodology are not publicly available. There are also some open source or academic attempts that I will use as a comparison with my work. \\

\noindent These projects target three main categories:

\begin{itemize}
  \item Web3 and dApps developers, that need data to feed their applications.
  \item Data analysts, that need to analyse historical blockchain data. 
  \item Blockchain users, that need to see the results of transactions.
\end{itemize}

\noindent In the next sections I'll list the state of the art tools available.

\section{Etherscan}

Etherscan \footnote{https://etherscan.io/} is the reference point for accessing data about the Ethereum blockchain. It’s the most used explorer that lets people browse historical data trough a web interface. Here users can easily explore transactions, internal calls, token transfers and everything else related to the Ethereum protocol. It’s useful for inspecting singular operations, but it can’t be used for large scale analysis.

One of the most important service they offer is the verification of smart contracts, they host 461,261\footnote{This data was calculated using the CSV file exported from https://etherscan.io/chart/verified-contracts} source codes (as of 17 May 2023) that have been verified to match exactly the deployed bytecode on the  Ethereum chain. 

The process of verification consists in providing Etherscan the exact source code of a Smart Contract, the version of the compiler used, the license selected and the contract address to verify. With this information, Etherscan will try to compile the given data and check if the resulting bytecode equals the one deployed on the blockchain. There are plugins for the most used development tools like Remix or Hardhat that ease this process of verification.

This data is extremely helpful for understanding the semantic of the code deployed, since it's very hard to get useful insights by just looking at the raw bytecode. Many studies are based on these verified contracts.

Etherscan has evolved from being just an Ethereum explorer. They used the huge amount of data they have to create API endpoints and offer these data to users for a fee. These API endpoints include the standard Ethereum JSON-RPC interface and many more advanced methods, like tokens logic and specific indexes not supported by the standard RPC. 

On top of that, they also provide live and interactive charts\footnote{Available at https://etherscan.io/charts} about historical Ethereum data.

The same company that is behind the Ethereum Etherscan explorer applied the same logic and technology to other EVM compatible chains, like Polygon\footnote{https://polygonscan.com/} or BNB Smart Chain\footnote{https://bscscan.com/}.

It's important to note that, although they provide almost all the possible available Ethereum data, they haven't shared technical details about how this data is extracted or how it is indexed, users need to trust the company. Another problem is that using their data via the API for large scale analysis is unfeasible since it would be too expensive.


\section{The Graph}

The Graph~\cite{the-graph} is a decentralized indexing protocol for blockchain data. It allows users to get structured on-chain data from other users via a GraphQL\footnote{GraphQL is an open source query language created by Facebook.} interface. 

All the data is organized is so called \textbf{subgraphs} that are independent data collections that index a small subset of a blockchain network. A common pattern is that a subgraph indexes data from one or a set of few smart contracts all part of a common protocol, like Uniswap\footnote{\url{https://uniswap.org/}}. All the available subgraphs can be found on the explorer\footnote{https://thegraph.com/explorer}.

The underlying protocol is composed of multiple actors:

\begin{itemize}
  \item Developers: people with technical knowledge that develop the needed code for creating and maintaining the indexes. As of now, the most important pieces of code needed are the mappings from Ethereum events to the stored data (written in AssemblyScript\footnote{https://www.assemblyscript.org/}, a Typescript-like language that is compiled to WebAssembly) and the subgraph manifest, a structured description of all the parts needed by the subgraph in YAML format.  
  \item Indexers: they are responsible for operating a node, this implies indexing the data following a subgraph specification and serving queries.
  \item Curators: they are in charge of finding the best subgraphs to be indexed.
  \item Delegators: they secure the network by locking economical value to certain indexers they choose, giving them the possibility to serve more queries.
\end{itemize}

All these actors are economically motivated to perform well, this is achieved via a token economy where the GRT is the currency. It is implemented on the Ethereum chain with a standard ERC20 smart contract\footnote{https://etherscan.io/token/0xc944e90c64b2c07662a292be6244bdf05cda44a7}.

In order to index and serve queries, indexers have to stake at least 100,000 GRT tokens (roughly equal to 12K USD with the current change). These tokens can be slashed in case the indexer behaves maliciously. The more tokens the indexer stakes and the more queries it can serve. At the same time, indexers are rewarded with GRT tokens in two ways: query fees and annual rewards based on amount of queries served.

According to the specification of the subgraph file\footnote{https://github.com/graphprotocol/graph-node/blob/master/docs/subgraph-manifest.md\#15-data-source}, the only allowed source of data are Ethereum contracts and mappings are restricted to Events. Image \ref{fig:the-graph-data-flow} shows the flow of data in the protocol. In most cases, this is enough for dApps, since typically all the smart contracts are written in such a way that they emit events when things happen. 

On the other hand, it is not possible to index all the other kind of information for performing other analysis, such as block data, contract deployments, transactions, contracts destruction, etc. It is also not possible to extract data that was not meant to be extracted, since the emitted events are pieces of information that the developers of the smart contracts explicitly wanted to expose and index. 


\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{Figures/graph-dataflow.png}
  \caption[The Graph data flow]{Data flow of The Graph indexing protocol\protect\footnotemark.}
  \label{fig:the-graph-data-flow}
\end{figure}

\footnotetext{Source: https://thegraph.com/docs/en/about/}


As of today, the protocol still relies on a central hosted service that uses the subgraphs logic and code to index data, but queries are served from this centralized server for free. This should change in 2023, the network should slowly migrate from this centralized service to the decentralized protocol once the quality of the service will be comparable \footnote{https://thegraph.academy/developers/sunsetting-the-hosted-service/}.

The Graph is the first attempt to decentralize indexing of blockchain data, it's still a project with a lot of work behind the scene. It's the most promising mechanism to make Web3 and dApps not dependant from centralized data ingestion services.


\section{Ethereum-ETL}

Ethereum-ETL\footnote{https://github.com/blockchain-etl/ethereum-etl} is an open source tool for extracting data from the Ethereum blockchain following the \textbf{Extract-Transform-Load} pattern. It's written in Python and can be used trough a CLI.

\noindent Raw data can be extracted to CSV or JSON files using these commands:

\begin{itemize}
    \item \verb|export_blocks_and_transactions|: it calls \verb|eth_getBlockByNumber| RPC and maps the response to two files containing blocks and transactions.
    \item \verb|export_token_transfers|: it calls \verb|eth_getFilterlogs| applying a filter with the first topic set to \verb|0xddf252...23b3ef|, the Keccak-256 hash of the ERC20 and ERC721 Transfer event signature. Transfers are stored in a single file without distinction between ERC20 or ERC721.   
    \item \verb|export_traces|: it stores the internal transactions calling the \verb|trace_block| method.
\end{itemize}

\noindent All other kind of data can be extracted starting from the files that these previous commands store. It is possible to further extract:

\begin{itemize}
    \item Transaction receipts and logs starting from transaction hashes exported from \verb|export_blocks_and_transactions|.
    \item Contracts data, starting from the traces stored with \verb|export_traces|.
    \item Token contracts with metadata, starting from contracts (this requires two previous step of extraction). 
\end{itemize}

\subsection{Google BigQuery public dataset}

\noindent Ethereum-ETL also allows for streaming these data from an Ethereum node to the console. This functionality is used to ingest data into the popular Ethereum Google BigQuery public dataset\footnote{https://cloud.google.com/blog/products/data-analytics/ethereum-bigquery-public-dataset-smart-contract-analytics}. This dataset is organized in six tables that I'll report here since it's a good representation of the raw data that can be extracted from an Ethereum node.

\begin{table}[H]
\centering
    \begin{tabular}  { m{6cm} m{3cm} m{3cm} } 
    \toprule
    \textbf{Attribute} & \textbf{Type} & \textbf{Required} \\
    \midrule
    timestamp & Timestamp & Yes\\ %& The timestamp for when the block was collated	\\
    number & Integer & Yes\\ %& & The block number \\
    hash & String & Yes\\ %& & Hash of the block \\
    parent\_hash & String & No\\ %& & Hash of the parent block\\
    nonce & String & Yes\\ %& Hash of the generated proof-of-work \\
    sha3\_uncles & String & No\\ %& & SHA3 of the uncles data in the block \\
    logs\_bloom & String & No\\ %& & The bloom filter for the logs of the block \\
    transaction\_root & String & No\\ %& & The root of the transaction trie of the block \\
    state\_root& String & No\\ %& & The root of the final state trie of the block \\
    receipt\_root & String & No\\ %& & The root of the receipts trie of the block \\
    miner & String & No\\ %& & The address of the beneficiary to whom the mining rewards were given \\
    difficulty & Numeric & No\\ %& & Integer of the difficulty for this block\\
    total\_difficulty & Numeric & No\\ %& & Integer of the total difficulty of the chain until this block\\
    size & Integer & No\\ %& & The size of this block in bytes\\
    extra\_data & String & No\\ %& & The extra data field of this block\\
    gas\_limit & Integer & No\\ %& & The maximum gas allowed in this block\\
    gas\_used & Integer & No\\ %& & The total used gas by all transactions in this block \\
    transaction\_count & Integer & No\\ %& & The number of transactions in the block \\
    base\_fee\_per\_gas & Integer & No\\ %& & Protocol base fee per gas, which can move up or down \\
    withdrawals\_root & String & No\\ %& & The root of the withdrawal trie of the block\\
    withdrawals & Record & Repeated\\ %& & Validator withdrawals \\
    \quad- index & \quad- Integer & \quad- No\\ %& &  \\
    \quad- validator\_index & \quad- Integer & \quad- No\\ %& & \\
    \quad- address & \quad- Integer & \quad- No\\ %& & \\
    \quad- amount & \quad- Integer & \quad- No\\ %& & \\
    \bottomrule
\end{tabular}
\caption[Google BigQuery \texttt{Blocks} table]{Description of table Blocks on the Google BigQuery public dataset, from the official docs.}
\label{table:bigquery-blocks}
\end{table}

\begin{table}[H]
\centering
    \begin{tabular}  { m{6cm} m{3cm} m{3cm} } 
    \toprule
    \textbf{Attribute} & \textbf{Type} & \textbf{Required} \\
    \midrule
    log\_index & Integer	& Yes \\
    transaction\_hash & String & Yes \\
    transaction\_index & Integer & Yes	\\		
    address & String & No \\
    data & String & No \\
    topics & String & Repeated \\
    block\_timestamp & Timestamp & Yes \\ 
    block\_number & Integer & Yes \\
    block\_hash & String & Yes \\
    \bottomrule
\end{tabular}
\caption[Google BigQuery \texttt{Logs} table]{Description of table Logs on the Google BigQuery public dataset, from the official docs.}
\label{table:bigquery-logs}
\end{table}

\begin{table}[H]
\centering
    \begin{tabular}  { m{6cm} m{3cm} m{3cm} } 
    \toprule
    \textbf{Attribute} & \textbf{Type} & \textbf{Required} \\
    \midrule
    address & String & Yes \\
    bytecode & String & No \\
    function\_signature & String & repeated \\
    is\_erc20 & Boolean & No \\
    is\_erc721 & Boolean & No \\
    block\_timestamp & Timestamp & Yes \\
    block\_number & Integer & Yes \\
    block\_hash & String & Yes \\
    \bottomrule
\end{tabular}
\caption[Google BigQuery \texttt{Contracts} table]{Description of table Contracts on the Google BigQuery public dataset, from the official docs.}
\label{table:bigquery-contracts}
\end{table}

\begin{table}[H]
\centering
    \begin{tabular}  { m{6cm} m{3cm} m{3cm} } 
    \toprule
    \textbf{Attribute} & \textbf{Type} & \textbf{Required} \\
    \midrule
    transaction\_hash & String &	No	\\			
    transaction\_index & Integer	& No	\\			
    from\_address & String &	No \\	
    to\_address & String & No \\				
    value & Numeric &	No		\\		
    input & String &	No		\\		
    output & String	& No		\\		
    trace\_type & String &	Yes		\\		
    call\_type & String	& No	\\
    reward\_type & String	& No\\
    gas & Integer	& No\\
    gas\_used & Integer	& No\\
    subtraces & Integer	& No\\
    trace\_address & String	& No		\\
    error & String	& No		\\
    status & Integer	& No	\\		
    block\_timestamp & Timestamp	& Yes		\\		
    block\_number & Integer	& Yes			\\	
    block\_hash & String	& Yes	\\			
    trace\_id & String	& No \\
    \bottomrule
\end{tabular}
\caption[Google BigQuery \texttt{Traces} table]{Description of table Traces on the Google BigQuery public dataset, from the official docs.}
\label{table:bigquery-traces}
\end{table}

\begin{table}[H]
\centering
    \begin{tabular}  { m{6cm} m{3cm} m{3cm} } 
    \toprule
    \textbf{Attribute} & \textbf{Type} & \textbf{Required} \\
    \midrule
    token\_address & String & Yes	 \\			
    from\_address & String & No \\
    to\_address & String	& No	\\			
    value & String	& No	\\
    \bottomrule
\end{tabular}
\caption[Google BigQuery \texttt{Token\_transfers} table]{Description of table Token\_transfers on the Google BigQuery public dataset, from the official docs.}
\label{table:bigquery-transfers}
\end{table}

\begin{table}[H]
\centering
    \begin{tabular}  { m{6cm} m{3cm} m{3cm} } 
    \toprule
    \textbf{Attribute} & \textbf{Type} & \textbf{Required} \\
    \midrule
    hash & String	& Yes		\\		
    nonce & Integer	& Yes \\				
    transaction\_index & Integer	& Yes		\\		
    from\_address & String	& Yes			\\	
    to\_address & String	& No		\\		
    value & Numeric	& No \\				
    gas &  Integer	& No \\				 
    gas\_price &  Integer &	No \\				
    input &  String	& No \\				
    receipt\_cumulative\_gas\_used & Integer & 	No \\				
    receipt\_gas\_used  & Integer	& No \\				
    receipt\_contract\_address & String	& No \\				
    receipt\_root &  String & 	No \\				
    receipt\_status  & Integer & 	No \\				
    block\_timestamp  & Timestamp	 & Yes \\				
    block\_number  & Integer & 	Yes \\				
    block\_hash & String &	Yes \\				
    max\_fee\_per\_gas & Integer	& No \\	
    max\_priority\_fee\_per\_gas & Integer	& No \\		
    transaction\_type & Integer	& No				\\
    receipt\_effective\_gas\_price & Integer	& No	\\
    \bottomrule
\end{tabular}
\caption[Google BigQuery \texttt{Transactions} table]{Description of table Transactions on the Google BigQuery public dataset, from the official docs.}
\label{table:bigquery-transactions}
\end{table}

It is possible to query these tables using SQL syntax, they can be joined on equal fields. There is the possibility to query this database for free up to a certain monthly limit of processing storage and amount of data extracted. 

I'll compare Ethereum-ETL with my work in the next chapters.

\section{Dune analytics}

Dune Analytics\footnote{https://dune.com/home} is a company that provides tools to query and visualize data from multiple blockchains. They support Bitcoin, Solana, Ethereum and other 8 EVM chains.\\

Their application is web-based, from there it is possible to create queries using SQL syntax and visualize results in interactive charts. Multiple queries can be collected together to create dashboards\footnote{Example of a dashboard about history of Ethereum: https://dune.com/hildobby/ethereum}.  

\subsection{Data architecture}

Blockchain data was initially managed using PostgreSQL and SparkSQL until 2022, year in which they released their own engine called DuneSQL \footnote{https://dune.com/blog/dune-engine-v2}, migration to this technology is currently ongoing. \\

They started storing and indexing data with PostgreSQL.
In this DBMS entries are stored in pages following a row-oriented strategy, which means that all the attributes of a row are stored adjacently. This is beneficial when it's necessary to retrieve all the attributes of a row while querying. However, it leads to poor performance when filtering for specific attributes, since the database has to load many bytes containing irrelevant data (i.e., all the other attributes of the row that are not used). To optimize this, it is possible to create indexes on columns. These indexes will provide very good performance, but can be hard and slow to maintain, specially with huge amount of data. At Dune Analytics they tried this approach with traditional relational database combined with indexes, but they had to drop it stating that "the size of the datasets were so huge that the database was struggling to fully support them"\footnote{Source: https://dune.com/blog/introducing-dune-sql}. \\

So they came up with DuneSQL, a query engine built specifically for managing blockchain data. It's a fork of Trino, an open source distributed SQL query engine designed to query data from heterogeneous sources. The DuneSQL fork is not open source, so technical information can only be deducted from their blog articles or statements on the support community. The main features they added are \textit{varbinary} data type for storing addresses and hashes, as well as \textit{uint256} support for EVM data. \\

Data is physically stored on AWS S3 buckets using the Apache Parquet storage format\footnote{https://github.com/apache/parquet-format}. This storage system is a mix between column and row oriented. Data is split in files based on rows, inside these files, there are further row groups and inside these groups data is divided by columns. Image \ref{fig:parquet-structure} visualize this concept. \\

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{Figures/parquet-structure.png}
  \caption[Parquet storage format structure]{File structure in the Parquet storage format\protect\footnotemark.}
  \label{fig:parquet-structure}
\end{figure}

\footnotetext{Source: https://github.com/apache/parquet-format}

Indexing is done using the Parquet file's metadata. At the end of each file there's a part in which are stored min/max values of all the column values inside a page, as shown in image \ref{fig:parquet-index}. This information allows the database to skip reading the whole chunk if values are not in the desired range. While working great for certain types of data, it isn't very useful with strings, especially if they are not sorted. That's the reason why even a simple query like \ref{lst:dune-query} can take several minutes to run. \\

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{Figures/parquet-index.jpg}
  \caption[Parquet ColumnIndex on Dune Analytics]{Parquet min/max column index structure \protect\footnotemark. }
  \label{fig:parquet-index}
\end{figure}

\footnotetext{Source: https://dune.com/docs/query/storage/}

\begin{lstlisting}[language=SQL,caption={Shortened simple query on DuneSQL that took 3 minutes to run},label={lst:dune-query},captionpos=b]
Select * from ethereum.transactions
where hash = 0x9ef65fe51...ff74219e
\end{lstlisting}

\subsection{Available data}

On Dune Analytics, blockchain data is available in multiple layers:

\begin{itemize}
    \item \textit{Raw tables}: these tables contain raw data as it is stored on the various blockchains, without modifications. For EVM chains this means Blocks, Logs, Traces and Transactions.
    \item \textit{Decoded tables}: verified contracts receive their own tables on which data is stored in a more readable way. Each event and function that is present in the contract's ABI corresponds to a table on Dune with the following name pattern: \texttt{project\_chain.contract\_[evt/call]\_evtOrCallName}. Inside this table there will be columns for each of the event or function attributes, with the relative names. For example, all the \textit{cryptokitties} \texttt{Transfer} events are stored in a table called \texttt{cryptokitties\_ethereum.KittyCore\_evt\_Transfer} that will have, among other contextual information, three columns: \textit{from}/\textit{to} addresses and \textit{tokenId}.
    \item \textit{Spellbooks}: these tables are abstractions over raw and decoded data that give users an easy way to retrieve information without diving into the technicalities of complex decentralized protocols. They're open source and the community can contribute creating new spellbooks on the Github Repository\footnote{https://github.com/duneanalytics/spellbook}. The main power of spellbooks is to put together data from heterogeneous sources to gather useful insights. The most clear example is the \texttt{dex.trades}\footnote{https://github.com/duneanalytics/spellbook/blob/main/models/dex/dex\_trades.sql} spellbook, that puts together all the trades made on all decentralized exchanges of all the blockchains present on Dune Analytics.
\end{itemize}

\section{XBlock-ETH}

Zheng et al.~\cite{xblock-eth} released a dataset containing raw Ethereum data and described the framework used for getting it, called XBlock-ETH. It was released in 2019 and data is periodically updated in chunks of 500,000 blocks, currently it covers blocks from 0 to 16,499,999. It is divided in different smaller datasets: \textit{Block}, \textit{Block Transaction}, \textit{Internal Transaction}, \textit{Contract Info}, \textit{ERC20 Transaction}, \textit{ERC721 Transaction}, \textit{Token Info}. Data can be downloaded from their website\footnote{https://xblock.pro/xblock-eth.html} in CSV format.

This is a useful resource for getting Ethereum data without having the possibility to run a node; however it lacks important information such as logs and receipts. 

It's not easy to use, since the massive amount of data in the CSV files must be parsed and cannot be easily queried or indexed, further transformation steps are needed. There are some Python scripts available on Github\footnote{https://github.com/tczpl/XBlock-ETH} for downloading and analysing the data, but the code used for the extraction is not open source, so it's not possible to know exactly how information was retrieved.

\section{Data-ether}

DateEther \cite{dataether} is a framework presented by Chen et al. for extracting and indexing Ethereum data. They tried a different approach for executing this task: they modified a Geth\footnote{https://geth.ethereum.org/} node to record and store data during the initial synchronization phase. They used ElasticSearch\footnote{https://www.elastic.co/} for indexing and exploring data, but by the time of their research the size of data was relatively small compared to now.

While extracting data by modifying a node source code was efficient back in 2019, now Ethereum nodes have evolved and there are different and faster RPCs for getting internal transactions. Their way of extracting data was compared against \texttt{debug\_traceTransaction} RPC of Geth, claiming to be 18.6x faster, but it wasn't compare against \texttt{debug\_traceBlock} or \texttt{trace\_block} of Erigon\footnote{https://github.com/ledgerwatch/erigon}. From my work, using Erigon's \texttt{trace\_block} RPC, I managed to get and loop trough all the internal transactions in around 9 hours (40 including transactions and logs). Doing that while synchronizing a node would have required at least 3-4 days. 

Extracting data by modifying source code has another drawback: maintainability. Just Geth itself received 173 releases\footnote{https://github.com/ethereum/go-ethereum/releases}, modifying its source code would mean having to merge the code and resolving eventual conflicts every time a new release is published. This is not a problem using Ethereum RPC APIs since they don't change after upgrades of the nodes.

\section{Web3 providers}

The term "Web3 provider" is commonly used to refer to companies that offer access to Ethereum RPC API, avoiding developers of Web3 dApps the costs of running a node. They're included in this chapter since the majority of them also provide access to indexed and interpreted data. Some popular web3 providers include Alchemy\footnote{https://www.alchemy.com/}, Infura\footnote{https://www.infura.io/}, Quicknode\footnote{https://www.quicknode.com/} and Chainstack\footnote{https://chainstack.com/}.

All of them have endpoints for getting NFTs data, it's possible to get all NFTs owned by an address by just calling an API instead of having to analyze all the chain. Chainstack also hosts on their servers all the supgraphs of "The Graph" protocol, offering users the possibility to use a more stable service instead of the decentralized alterntive.

It is important to note that the web3 providers are profit-oriented companies. While they provide an important service in the world of dApps, it's essential to consider their cost implications. Using their services for analyzing historical data can be very expensive, since billing is done based on number of API requests to their server.

Another important aspect to consider is the transparency of these services. As profit-oriented entities, these web3 providers do not necessarily make their source code and methodology open source. This means that developers relying on their services need to trust the data they receive. 

Depending on centralized companies poses also a risk to the actual decentralization of the web3. Access to the blockchain infrastructure is concentrated on a few companies, as noted by Wang et al.~\cite{wang2022exploring}.

\section{Comparison}

I summarized in table \ref{table:tools-comparison} the main differences between the tools in terms of \textit{primary target}, \textit{transparency} and \textit{price}.

\begin{table}[ht!]
\centering
    \begin{threeparttable}
    \begin{tabular}  { m{3cm} m{3cm} m{1.5cm} m{5cm} } 
    \toprule
    \textbf{Tool} & \textbf{Target} & \textbf{Open source} & \textbf{Price}  \\
    \midrule
    Etherscan    & Blockchain users  & No & Free explorer, paid apis  \\[2.3ex]
    The Graph     & Web3 developers & Yes & Billing based on usage  \\[1.3ex]
    Ethereum-ETL     & Data analysts & Yes & Free  \\[1.3ex]
    Dune Analytics      & Data analysts & No & Based on query credits, it has free plan \\[2.6ex]
    XBlock-ETH  & Data analysts & No\tnote{*} & Free   \\[1.3ex]
    DataEther  & Data analysts & No\tnote{*} & Free   \\[1.3ex]
    Web3 providers      & Web3 developers & No & Billing on usage, they have free plans   \\[1.6ex]
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
      \item[*] Source code not available, but they released technical papers describing the software.
      \end{tablenotes}
    \end{threeparttable}
\caption[State of the art tools comparison]{Comparison of state-of-the art tools for management of blockchain data.}
\label{table:tools-comparison}
\end{table}
